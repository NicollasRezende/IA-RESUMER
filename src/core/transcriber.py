"""
M√≥dulo de transcri√ß√£o aprimorado com melhor qualidade e robustez.
"""
import json
import time
from pathlib import Path
from typing import Dict, List, Optional, Union, Tuple
import warnings
import hashlib

import whisper
import torch
import numpy as np
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn
from rich.panel import Panel
from rich.table import Table

from .config import (
    WHISPER_CONFIG, 
    TRANSCRIPT_DIR, 
    get_model_info,
    OUTPUT_CONFIG,
    TEMP_DIR
)
from .audio_processor import AudioProcessor

warnings.filterwarnings("ignore", category=UserWarning)
console = Console()


class WhisperTranscriber:
    """Transcritor aprimorado com melhor qualidade e features avan√ßadas."""
    
    def __init__(self, model_size: Optional[str] = None, device: Optional[str] = None):
        """
        Inicializa o transcritor aprimorado.
        
        Args:
            model_size: Tamanho do modelo (tiny, base, small, medium, large)
            device: Dispositivo para execu√ß√£o (cpu, cuda)
        """
        self.model_size = model_size or WHISPER_CONFIG["model_size"]
        self.device = device or WHISPER_CONFIG["device"]
        self.model = None
        self.audio_processor = AudioProcessor()
        self._load_model()
        
    def _load_model(self):
        """Carrega o modelo Whisper com feedback visual."""
        model_info = get_model_info(self.model_size)
        
        console.print(f"\nüéôÔ∏è  Carregando modelo Whisper [bold cyan]{self.model_size}[/bold cyan]")
        console.print(f"   Tamanho: {model_info['size']} | VRAM necess√°ria: {model_info['vram']}")
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            console=console,
        ) as progress:
            task = progress.add_task("Baixando e carregando modelo...", total=None)
            
            start_time = time.time()
            self.model = whisper.load_model(
                self.model_size, 
                device=self.device,
                download_root=None
            )
            
            elapsed = time.time() - start_time
            progress.update(task, completed=True)
            
        console.print(f"‚úÖ Modelo carregado em {elapsed:.1f}s no dispositivo: [bold green]{self.device}[/bold green]\n")
    
    def transcribe_enhanced(
        self, 
        audio_path: Union[str, Path],
        language: Optional[str] = None,
        task: str = "transcribe",
        initial_prompt: Optional[str] = None,
        temperature: Union[float, List[float]] = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0],
        compression_ratio_threshold: float = 2.4,
        logprob_threshold: float = -1.0,
        no_speech_threshold: float = 0.6,
        condition_on_previous_text: bool = True,
        **kwargs
    ) -> Dict:
        """
        Transcri√ß√£o aprimorada com melhor qualidade e robustez.
        
        Args:
            audio_path: Caminho do arquivo de √°udio
            language: Idioma (None para detec√ß√£o autom√°tica)
            task: 'transcribe' ou 'translate'
            initial_prompt: Prompt para guiar o modelo
            temperature: Temperatura(s) para fallback em caso de falha
            compression_ratio_threshold: Threshold para detectar repeti√ß√µes
            logprob_threshold: Threshold para detectar baixa confian√ßa
            no_speech_threshold: Threshold para detectar sil√™ncio
            condition_on_previous_text: Usar contexto anterior
            
        Returns:
            Dict com transcri√ß√£o completa e metadados
        """
        audio_path = Path(audio_path)
        if not audio_path.exists():
            raise FileNotFoundError(f"Arquivo n√£o encontrado: {audio_path}")
        
        console.print(f"üéµ Processando: [bold]{audio_path.name}[/bold]")
        
        # Pr√©-processar √°udio se necess√°rio
        processed_path = self._preprocess_audio(audio_path)
        
        # Detectar idioma se n√£o especificado
        if not language:
            language = self.detect_language(processed_path)
        
        # Configurar par√¢metros otimizados
        params = {
            "language": language,
            "task": task,
            "fp16": WHISPER_CONFIG["fp16"] and self.device == "cuda",
            "verbose": False,  # Desabilitar verbose padr√£o
            "temperature": temperature,
            "compression_ratio_threshold": compression_ratio_threshold,
            "logprob_threshold": logprob_threshold,
            "no_speech_threshold": no_speech_threshold,
            "condition_on_previous_text": condition_on_previous_text,
            "word_timestamps": True,  # Sempre usar para melhor precis√£o
            "initial_prompt": initial_prompt or self._get_language_prompt(language),
            "beam_size": 5,  # Melhor qualidade
            "best_of": 5,   # M√∫ltiplas tentativas
            "patience": 1.0,
        }
        
        # Atualizar com kwargs
        params.update(kwargs)
        
        # Transcrever com estrat√©gia de chunks para arquivos longos
        audio_info = self.audio_processor.get_audio_info(processed_path)
        duration = audio_info.get("duration", 0)
        
        if duration > 300:  # Se maior que 5 minutos, usar chunks
            console.print(f"üìä √Åudio longo detectado ({duration:.1f}s), usando processamento em chunks...")
            result = self._transcribe_long_audio(processed_path, params)
        else:
            result = self._transcribe_single(processed_path, params)
        
        # P√≥s-processar resultado
        result = self._postprocess_transcription(result)
        
        # Limpar arquivo tempor√°rio se criado
        if processed_path != audio_path and processed_path.exists():
            processed_path.unlink()
        
        return result
    
    def _preprocess_audio(self, audio_path: Path) -> Path:
        """
        Pr√©-processa o √°udio para melhor qualidade de transcri√ß√£o.
        
        Args:
            audio_path: Caminho do √°udio original
            
        Returns:
            Path do √°udio processado
        """
        # Verificar se precisa normaliza√ß√£o
        audio_info = self.audio_processor.get_audio_info(audio_path)
        
        # Se j√° est√° no formato ideal, retornar
        if (audio_info.get("sample_rate") == 16000 and 
            audio_info.get("channels") == 1 and
            audio_path.suffix.lower() == '.wav'):
            return audio_path
        
        console.print("üîß Otimizando √°udio para transcri√ß√£o...")
        
        # Converter e normalizar
        processed_path = self.audio_processor.convert_to_wav(audio_path)
        normalized_path = self.audio_processor.normalize_audio(processed_path)
        
        # Remover arquivo intermedi√°rio
        if processed_path != audio_path and processed_path.exists():
            processed_path.unlink()
        
        return normalized_path
    
    def _transcribe_single(self, audio_path: Path, params: Dict) -> Dict:
        """
        Transcreve um √∫nico arquivo de √°udio.
        
        Args:
            audio_path: Caminho do √°udio
            params: Par√¢metros para transcri√ß√£o
            
        Returns:
            Resultado da transcri√ß√£o
        """
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            console=console,
        ) as progress:
            task = progress.add_task("Transcrevendo √°udio...", total=100)
            
            start_time = time.time()
            
            # Callback para progresso
            def callback(current, total):
                progress.update(task, completed=int((current / total) * 100))
            
            # Transcrever com Whisper
            result = self.model.transcribe(
                str(audio_path),
                **params,
                progress_callback=callback if hasattr(self.model, 'transcribe_with_progress') else None
            )
            
            progress.update(task, completed=100)
            elapsed = time.time() - start_time
        
        # Adicionar metadados
        result["processing_time"] = elapsed
        result["method"] = "single"
        
        return result
    
    def _transcribe_long_audio(self, audio_path: Path, params: Dict) -> Dict:
        """
        Transcreve √°udio longo usando estrat√©gia de chunks.
        
        Args:
            audio_path: Caminho do √°udio
            params: Par√¢metros para transcri√ß√£o
            
        Returns:
            Resultado combinado da transcri√ß√£o
        """
        # Dividir √°udio em chunks por sil√™ncio ou dura√ß√£o
        console.print("‚úÇÔ∏è  Dividindo √°udio em segmentos...")
        
        # Tentar dividir por sil√™ncio primeiro
        chunks = self.audio_processor.split_audio_by_silence(
            audio_path,
            min_silence_len=500,    # 500ms de sil√™ncio
            silence_thresh=-35,     # Threshold mais sens√≠vel
            keep_silence=250        # Manter 250ms de contexto
        )
        
        # Se n√£o encontrou sil√™ncios suficientes, dividir por tempo
        if len(chunks) < 2:
            chunks = self.audio_processor.split_audio_by_duration(
                audio_path,
                chunk_duration=240  # 4 minutos por chunk
            )
        
        console.print(f"üìö Processando {len(chunks)} segmentos...")
        
        # Transcrever cada chunk
        all_segments = []
        full_text = []
        total_duration = 0
        
        for i, chunk_path in enumerate(chunks):
            console.print(f"\nüé§ Segmento {i+1}/{len(chunks)}")
            
            # Usar prompt do segmento anterior para continuidade
            if i > 0 and all_segments:
                # Pegar √∫ltimas palavras do segmento anterior
                prev_text = all_segments[-1]["text"]
                last_words = " ".join(prev_text.split()[-10:])
                params["initial_prompt"] = f"...{last_words}"
            
            # Transcrever chunk
            chunk_result = self._transcribe_single(chunk_path, params.copy())
            
            # Ajustar timestamps
            if all_segments:
                time_offset = all_segments[-1]["end"]
                for segment in chunk_result.get("segments", []):
                    segment["start"] += time_offset
                    segment["end"] += time_offset
                    segment["id"] = len(all_segments) + segment["id"]
            
            # Adicionar aos resultados
            all_segments.extend(chunk_result.get("segments", []))
            full_text.append(chunk_result.get("text", "").strip())
            
            # Atualizar dura√ß√£o
            if chunk_result.get("segments"):
                total_duration = chunk_result["segments"][-1]["end"]
            
            # Limpar chunk tempor√°rio
            chunk_path.unlink()
        
        # Combinar resultados
        result = {
            "text": " ".join(full_text),
            "segments": all_segments,
            "language": params["language"],
            "duration": total_duration,
            "method": "chunks",
            "num_chunks": len(chunks)
        }
        
        return result
    
    def _postprocess_transcription(self, result: Dict) -> Dict:
        """
        P√≥s-processa a transcri√ß√£o para melhorar qualidade.
        
        Args:
            result: Resultado bruto da transcri√ß√£o
            
        Returns:
            Resultado processado
        """
        console.print("\nüîß Aplicando p√≥s-processamento...")
        
        # Remover repeti√ß√µes excessivas
        result["text"] = self._remove_repetitions(result["text"])
        
        # Limpar segmentos
        cleaned_segments = []
        for segment in result.get("segments", []):
            # Remover segmentos vazios ou muito curtos
            if len(segment["text"].strip()) < 3:
                continue
            
            # Limpar texto do segmento
            segment["text"] = self._clean_segment_text(segment["text"])
            
            # Calcular confian√ßa m√©dia se dispon√≠vel
            if "words" in segment:
                confidences = [w.get("probability", 1.0) for w in segment["words"]]
                segment["avg_confidence"] = sum(confidences) / len(confidences) if confidences else 1.0
            
            cleaned_segments.append(segment)
        
        result["segments"] = cleaned_segments
        
        # Adicionar estat√≠sticas de qualidade
        result["quality_metrics"] = self._calculate_quality_metrics(result)
        
        return result
    
    def _remove_repetitions(self, text: str) -> str:
        """Remove repeti√ß√µes excessivas do texto."""
        import re
        
        # Remover m√∫ltiplos espa√ßos
        text = re.sub(r'\s+', ' ', text)
        
        # Remover repeti√ß√µes de palavras (3+ vezes)
        text = re.sub(r'\b(\w+)(\s+\1){2,}\b', r'\1', text)
        
        # Remover m√∫ltiplos pontos consecutivos (mais de 3)
        text = re.sub(r'\.{4,}', '...', text)
        
        return text.strip()
    
    def _clean_segment_text(self, text: str) -> str:
        """Limpa o texto de um segmento."""
        # Remover espa√ßos extras
        text = " ".join(text.split())
        
        # Capitalizar primeira letra
        if text and text[0].islower():
            text = text[0].upper() + text[1:]
        
        # Adicionar pontua√ß√£o final se n√£o houver
        if text and text[-1] not in '.!?':
            text += '.'
        
        return text
    
    def _calculate_quality_metrics(self, result: Dict) -> Dict:
        """
        Calcula m√©tricas de qualidade da transcri√ß√£o.
        
        Args:
            result: Resultado da transcri√ß√£o
            
        Returns:
            Dict com m√©tricas de qualidade
        """
        segments = result.get("segments", [])
        
        metrics = {
            "total_segments": len(segments),
            "total_words": len(result["text"].split()),
            "avg_segment_confidence": 0,
            "low_confidence_segments": 0,
            "silence_ratio": 0,
        }
        
        if segments:
            # Calcular confian√ßa m√©dia
            confidences = [s.get("avg_confidence", 1.0) for s in segments]
            metrics["avg_segment_confidence"] = sum(confidences) / len(confidences)
            
            # Contar segmentos de baixa confian√ßa
            metrics["low_confidence_segments"] = sum(1 for c in confidences if c < 0.8)
            
            # Calcular propor√ß√£o de sil√™ncio
            total_duration = result.get("duration", 0)
            speech_duration = sum(s["end"] - s["start"] for s in segments)
            if total_duration > 0:
                metrics["silence_ratio"] = 1 - (speech_duration / total_duration)
        
        return metrics
    
    def _get_language_prompt(self, language: str) -> str:
        """
        Retorna um prompt apropriado para o idioma.
        
        Args:
            language: C√≥digo do idioma
            
        Returns:
            Prompt inicial para melhorar a transcri√ß√£o
        """
        prompts = {
            "pt": "Esta √© uma transcri√ß√£o em portugu√™s. O √°udio pode conter termos t√©cnicos.",
            "en": "This is an English transcription. The audio may contain technical terms.",
            "es": "Esta es una transcripci√≥n en espa√±ol. El audio puede contener t√©rminos t√©cnicos.",
        }
        
        return prompts.get(language, "")
    
    def transcribe_with_fallback(
        self,
        audio_path: Union[str, Path],
        models: List[str] = ["small", "medium", "large"],
        **kwargs
    ) -> Dict:
        """
        Tenta transcrever com m√∫ltiplos modelos em caso de falha.
        
        Args:
            audio_path: Caminho do √°udio
            models: Lista de modelos para tentar em ordem
            **kwargs: Argumentos para transcri√ß√£o
            
        Returns:
            Melhor resultado obtido
        """
        audio_path = Path(audio_path)
        best_result = None
        best_score = 0
        
        for model_size in models:
            try:
                console.print(f"\nüîÑ Tentando com modelo [bold]{model_size}[/bold]...")
                
                # Criar novo transcritor com o modelo
                transcriber = WhisperTranscriber(
                    model_size=model_size,
                    device=self.device
                )
                
                # Transcrever
                result = transcriber.transcribe_enhanced(audio_path, **kwargs)
                
                # Calcular score de qualidade
                metrics = result.get("quality_metrics", {})
                score = (
                    metrics.get("avg_segment_confidence", 0) * 0.5 +
                    (1 - metrics.get("silence_ratio", 1)) * 0.3 +
                    (1 - metrics.get("low_confidence_segments", 0) / max(metrics.get("total_segments", 1), 1)) * 0.2
                )
                
                console.print(f"   Score de qualidade: [bold]{score:.2f}[/bold]")
                
                # Manter melhor resultado
                if score > best_score:
                    best_result = result
                    best_score = score
                    best_result["model_used"] = model_size
                
                # Se score √© bom o suficiente, parar
                if score > 0.85:
                    break
                    
            except Exception as e:
                console.print(f"   [yellow]‚ö†Ô∏è  Falha com {model_size}: {e}[/yellow]")
                continue
        
        if not best_result:
            raise RuntimeError("Falha ao transcrever com todos os modelos")
        
        console.print(f"\n‚úÖ Melhor resultado com modelo: [bold green]{best_result['model_used']}[/bold green]")
        return best_result
    
    def generate_quality_report(self, result: Dict) -> None:
        """
        Gera um relat√≥rio de qualidade da transcri√ß√£o.
        
        Args:
            result: Resultado da transcri√ß√£o
        """
        console.print("\n" + "="*50)
        console.print("[bold cyan]RELAT√ìRIO DE QUALIDADE DA TRANSCRI√á√ÉO[/bold cyan]")
        console.print("="*50 + "\n")
        
        # Informa√ß√µes gerais
        metadata = result.get("metadata", {})
        metrics = result.get("quality_metrics", {})
        
        info_table = Table(show_header=False, box=None)
        info_table.add_column("M√©trica", style="cyan")
        info_table.add_column("Valor", style="white")
        
        info_table.add_row("Arquivo:", metadata.get("file", "N/A"))
        info_table.add_row("Dura√ß√£o:", f"{metadata.get('duration', 0):.1f}s")
        info_table.add_row("Modelo:", metadata.get("model", "N/A"))
        info_table.add_row("M√©todo:", result.get("method", "N/A"))
        info_table.add_row("Idioma:", result.get("language", "N/A"))
        
        console.print(info_table)
        console.print()
        
        # M√©tricas de qualidade
        quality_table = Table(title="M√©tricas de Qualidade", show_lines=True)
        quality_table.add_column("M√©trica", style="cyan")
        quality_table.add_column("Valor", style="white")
        quality_table.add_column("Status", style="white")
        
        # Confian√ßa m√©dia
        avg_conf = metrics.get("avg_segment_confidence", 0)
        conf_status = "[green]√ìtimo[/green]" if avg_conf > 0.9 else "[yellow]Bom[/yellow]" if avg_conf > 0.8 else "[red]Baixo[/red]"
        quality_table.add_row("Confian√ßa M√©dia", f"{avg_conf:.1%}", conf_status)
        
        # Segmentos de baixa confian√ßa
        low_conf = metrics.get("low_confidence_segments", 0)
        total_segs = metrics.get("total_segments", 1)
        low_conf_ratio = low_conf / total_segs if total_segs > 0 else 0
        low_conf_status = "[green]√ìtimo[/green]" if low_conf_ratio < 0.1 else "[yellow]Aceit√°vel[/yellow]" if low_conf_ratio < 0.2 else "[red]Alto[/red]"
        quality_table.add_row("Segmentos Baixa Confian√ßa", f"{low_conf}/{total_segs} ({low_conf_ratio:.1%})", low_conf_status)
        
        # Taxa de sil√™ncio
        silence_ratio = metrics.get("silence_ratio", 0)
        silence_status = "[green]Normal[/green]" if silence_ratio < 0.3 else "[yellow]Alto[/yellow]" if silence_ratio < 0.5 else "[red]Muito Alto[/red]"
        quality_table.add_row("Taxa de Sil√™ncio", f"{silence_ratio:.1%}", silence_status)
        
        # Total de palavras
        total_words = metrics.get("total_words", 0)
        quality_table.add_row("Total de Palavras", str(total_words), "[green]OK[/green]")
        
        console.print(quality_table)
        
        # Recomenda√ß√µes
        console.print("\n[bold]Recomenda√ß√µes:[/bold]")
        
        if avg_conf < 0.8:
            console.print("‚Ä¢ [yellow]Considere usar um modelo maior para melhor qualidade[/yellow]")
        
        if low_conf_ratio > 0.2:
            console.print("‚Ä¢ [yellow]Muitos segmentos com baixa confian√ßa - verifique a qualidade do √°udio[/yellow]")
        
        if silence_ratio > 0.5:
            console.print("‚Ä¢ [yellow]Alto √≠ndice de sil√™ncio - considere editar o √°udio antes da transcri√ß√£o[/yellow]")
        
        if total_words < 50:
            console.print("‚Ä¢ [yellow]Poucas palavras detectadas - verifique se o √°udio cont√©m fala[/yellow]")
        
        console.print("\n" + "="*50)